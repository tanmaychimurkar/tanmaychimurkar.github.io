[{"content":"Why and What‚ÅâÔ∏è Ô∏èIn Software Development cycle, developers of every level have either heard or said the following:\nIt works on my machine, I am not sure why it isn\u0026rsquo;t running on yours\nDifferent software projects have different dependencies, that only keep going uphill as the product itself evolves. When this happens, we need to make sure all interactions between different components have been done correctly in order for the whole application to come together and run.\nThis is the very scenario where things can fall apart very easily!\nWhy Dockerü§î In scenarios like mentioned above, wouldn\u0026rsquo;t it be useful to have a tool that makes sure that a software is bundled in a way that is irrespective of the base dependencies it requires to be built on?\nAs I primarily work in Pythonüêç, I would like to throw in an analogy. This abstraction tool can be thought of like a virtual environment, with each project having its own separate dependencies which do not interfere with a different project\u0026rsquo;s virtual environment, and every environment satisfying all the module level dependencies that the project needs.\nIn short, the tool that we need should have isolation and dependency matched as per the part of the project that is running on it!\nHaving such a tool to manage whole software, such that it handles all the base dependencies like that of the OS and any other package level dependencies are managed for us would be great, wouldn\u0026rsquo;t it?\nWhat is Dockerüê≥ In-line with the objectives that we want the above-mentioned tool to perform, let\u0026rsquo;s take a look at Docker.\nDocker helps abstract away the dependencies for components between varying OS, software, and hardware requirements, and lets every part of the project run in an isolated environment.\nFeels like technical mumbo jumbo? Well let\u0026rsquo;s take another go at understanding this. The abstraction that Docker provides can be thought of in the following way:\nImagine breaking the whole software into multiple smaller pieces, with each piece handling one part of some logic. This is what we call an isolated part of the project. Each part will naturally have a set of requirements in the forms of packages or configurations for it to be able to run correctly. This is the dependency that the project has. Running our project using Docker will abstract away the above said requirements for other people, and anybody running our project will use the configurations we have set when moving the project to Docker. I hope it is more or less clear what Docker does. If not, fret not! Things should become clearer as we go further with terminology.\nTerminologyüìñ So far, we have been trying to understand what Docker does in our own analogy and terms. However, things can go out of hand if one does not follow a standard terminologyüê±.\nLet\u0026rsquo;s dive in the terminology that will make our life easier to follow.\nObviously, one should at the end refer to the official glossary mentioned by Docker. However, to quickly get us started, I have mentioned some base terminologies below.\nDocker Image‚úíÔ∏è Since we need to create an isolated environment, we first need to decide what will the base of such an environment have. This base that each isolated environment needs is defined in a Docker image, which contains union of layered filesystems stacked on top of each other.\nWe can think of image as a small pre-compiled software on which we can run our assigned part of the project, and using an image is like installing Python or Ubuntu on your own machine.\nBecause of the awesome strength of the Docker community, they decided to keep a collection of the images that people need. This collection of images is listed on the Docker Hub, and we can find all sorts of images, like Alpine Linux and Python.\nNote: Although the Docker hub has many images readily available, we can also create our own Docker images. And as we read above that images are union of layered filesystems, our own image will also be a layer over some base image.\nA more detailed explanation of Docker Images is here.\nDocker Containerüì¶ So we saw that image in Docker is a base system that we want to run our piece of the project. When we run such an image, we get a Docker container.\nIn short, a Docker container is a running instance of a Docker image.\nA container always needs an image, and along with it, we sometimes pass a set of execution parameters such that the container has the parameters that we want, for example it\u0026rsquo;s name.\nA more detailed explanation of Docker containers is here.\nSumming it up!üí° What we saw so far is a very basic understanding of what Docker is, why we might need it, and what are the basic things we need to understand for breaking our project into small running containers. These small running containers can also be thought of as very small virtual machines, and this will be more clear when we look at the Docker Containers post. In the next part, let\u0026rsquo;s look at Docker Images in a bit more detail and how we can create our image. üò∫\n","permalink":"https://tanmaychimurkar.github.io/posts/docker/docker_intro/","summary":"Why and What‚ÅâÔ∏è Ô∏èIn Software Development cycle, developers of every level have either heard or said the following:\nIt works on my machine, I am not sure why it isn\u0026rsquo;t running on yours\nDifferent software projects have different dependencies, that only keep going uphill as the product itself evolves. When this happens, we need to make sure all interactions between different components have been done correctly in order for the whole application to come together and run.","title":"The basics of Docker to get started"},{"content":"What is a configuration file‚ÅâÔ∏è A configuration file is a text file that contains various settings and options that control the behavior of a software program or system. The file is usually written in a specific format, such as JSON, YAML,INI, or TOML and is read by the program or system at startup. The settings in the configuration file can be modified to change the behavior of the program or system without having to change the code.\nA configuration file can include options such as the location of other files or resources, user preferences, settings for different environments (e.g. development, production), and various other parameters that control how the program or system behaves. Configuration files are often used to configure servers, network devices, and other types of systems.\nFor example, a web server might have a configuration file that specifies the IP address and port it should listen on, the document root for the web server, and other settings related to security, logging, and performance.\nBy using configuration files, administrators and developers can easily configure and customize the behavior of a program or system without having to modify the code or recompile the program.\nWhat is the JSON configuration file‚ùì A JSON (JavaScript Object Notation) configuration file is a file format that stores simple data structures and objects in a human-readable, easy-to-access manner. JSON files are often used to store configuration settings and data for software applications, and can be easily read and edited by both humans and machines. They are typically saved with a .JSON file extension and use a syntax similar to that of JavaScript objects. JSON files can be easily parsed and processed by many programming languages, making them a popular choice for configuration files.\nExample of a JSON file might look like this:\n{ \u0026#34;key1\u0026#34;: \u0026#34;value1\u0026#34;, \u0026#34;key2\u0026#34;: \u0026#34;value2\u0026#34;, \u0026#34;key3\u0026#34;: { \u0026#34;subkey1\u0026#34;: \u0026#34;subvalue1\u0026#34;, \u0026#34;subkey2\u0026#34;: \u0026#34;subvalue2\u0026#34; }, \u0026#34;key4\u0026#34;: [ \u0026#34;arrayvalue1\u0026#34;, \u0026#34;arrayvalue2\u0026#34;, \u0026#34;arrayvalue3\u0026#34; ] } Things to note about a JSON file JSON files consist of key-value pairs, similar to a JavaScript object.\nThe keys are strings, enclosed in double quotes, and the values can be either \u0026lsquo;strings\u0026rsquo;, \u0026rsquo;numbers\u0026rsquo;, \u0026lsquo;objects\u0026rsquo; (enclosed in curly braces), \u0026lsquo;arrays\u0026rsquo; (enclosed in square brackets), or special values \u0026rsquo;true\u0026rsquo;, \u0026lsquo;false\u0026rsquo;, and \u0026rsquo;null\u0026rsquo;.\nString values must also be enclosed in double quotes (\u0026quot;).\nWhat is the YAML configuration file‚ùì YAML (short for \u0026ldquo;YAML Ain\u0026rsquo;t Markup Language\u0026rdquo;) is a human-readable data serialization format that is often used for configuration files, data exchange, and data structuring. It is designed to be easy to read and write, and is often used as an alternative to XML or JSON.\nA YAML file is a text file that contains data structured in a specific way. It uses indentation and whitespace to indicate the structure of the data, making it easy to read and understand. The basic building blocks of a YAML file are key-value pairs, which are separated by a colon. Lists and arrays are also supported, and can be represented using a simple syntax.\nExample of a simple YAML file might look like this:\nname: John Doe age: 35 address: street: 123 Main St city: Anytown state: CA zip: 12345 YAML files are often used for configuration files because they are easy to read and understand, and are less verbose than other formats such as XML or JSON. They are also used in various tools and frameworks such as Ansible, Kubernetes, and Docker Compose, where they are used to define the configuration of the system.\nIt\u0026rsquo;s important to note that YAML is case-sensitive and indentation is important, so if you don\u0026rsquo;t use proper indentation or typo the key names it will cause errors.\nThings to note about a YAML file In a YAML file, indentation is used to indicate the hierarchical structure of the data. Each level of indentation represents a new level in the data hierarchy.\nFor example, in the above YAML file, the \u0026rsquo;name\u0026rsquo;, \u0026lsquo;age\u0026rsquo;, and \u0026lsquo;address\u0026rsquo; keys are at the same level, and are considered to be siblings. The \u0026lsquo;street\u0026rsquo;, \u0026lsquo;city\u0026rsquo;, \u0026lsquo;state\u0026rsquo;, and \u0026lsquo;zip\u0026rsquo; keys are indented under the address key, indicating that they are child elements of the address element.\nIt is important to use consistent indentation throughout the YAML file. Typically, two spaces are used for each level of indentation, but it can vary. However, it is important to use the same number of spaces throughout the file, as YAML is sensitive to indentation. An error in indentation can cause the YAML file to be parsed incorrectly, resulting in errors when the file is read by the software that uses it.\nAlso, YAML files use the dash - to indicate the start of a list, for example:\nfruits: - apple - banana - orange In this example, fruits is a key and the indented items are the list of values for the key.\nWhat is the INI configuration file‚ùì An INI (Initialization) file is a configuration file used by some software applications to store settings and options. It is a simple text file that contains key-value pairs organized into sections. The format of an INI file is similar to that of a Windows INI file, although it is not limited to Windows.\nHere is an example of the structure of an INI file:\n[section1] key1=value1 key2=value2 [section2] key3=value3 key4=value4 Things to note about the INI file INI files are composed of sections, which are enclosed in square brackets [].\nEach section contains one or more key-value pairs, separated by an equal sign =. The key is a unique string that identifies the value, and the value is an arbitrary string that represents the data.\nComments can also be added by starting the line with a semi-colon ; or a hash #.\nINI files are often used to store application settings and options, such as user preferences or connection settings.\nUnlike JSON or XML, INI files are not formal, standard format and the syntax may vary depending on the application which uses it. They are often used for simple configuration settings where a more complex format is not required.\nNOTE: It\u0026rsquo;s important to note that the INI file format does not have a formal specification and thus the syntax may vary depending on the application that uses it. However, most implementations follow the above structure and this is a common format for INI files.\nWhat is the TOML configuration file‚ùì TOML (Tom\u0026rsquo;s Obvious, Minimal Language) is a configuration file format that is similar to INI files but with some additional features. It is designed to be easy to read and write, and is intended to be a more robust and consistent alternative to INI files.\nBelow is an example of how a TOML file looks like:\n[section1] key1 = \u0026#34;value1\u0026#34; key2 = \u0026#34;value2\u0026#34; [section2] key3 = \u0026#34;value3\u0026#34; key4 = \u0026#34;value4\u0026#34; Things to note about the TOML file TOML files are divided into sections, which are enclosed in square brackets [], just like the ini files.\nEach section contains one or more key-value pairs, separated by an equal sign =.\nThe keys must be unique within the section, and they are case-sensitive.\nThe values can be strings, integers, floats, booleans, and datetime.\nTOML files support inline comments, with the # symbol.\nTOML is designed to be easy to parse, and it is more consistent than INI files. It is also more expressive and can handle more complex data types. TOML is used as a configuration file format in a number of programming languages and projects, such as Rust and Cargo.\nCongratulationsüôåüéâü•≥üôåüéâü•≥ We saw in this post the common types of configuration files that are used to create a set of settings that a particular user or a software wants to be executed as defined in the configuration file.\nNow you are a pro at creating your own configuration file, and can use as many configuration files as you want in your software!! Until next time üòéüòéüòé\n","permalink":"https://tanmaychimurkar.github.io/posts/configurations/yaml_files/","summary":"What is a configuration file‚ÅâÔ∏è A configuration file is a text file that contains various settings and options that control the behavior of a software program or system. The file is usually written in a specific format, such as JSON, YAML,INI, or TOML and is read by the program or system at startup. The settings in the configuration file can be modified to change the behavior of the program or system without having to change the code.","title":"Configuration files"},{"content":"Docker Imagesüê≥ As we saw in the basics of Docker post, the base flow of every Docker container is an image. A Docker image, whether custom or pre-built, is absolutely necessary for us to be able to run anything using Docker.\nIn essence, a Docker image is a union of layered filesystem stacked on top of each other. This stands true for pre-built images as well as custom images that we might create using some pre-built image as a base.\nTriviaüí°: If every Docker image is a set of layered instructions, then how is the first Docker image created? What did the very first Docker image have as a layer to be built on top of?\nHoping that the description of the Docker images is clear now, let\u0026rsquo;s now take a look at our first Docker image. And what better to look at than the image of Ubuntu itself!?!?\nDocker Image on Hub Use this link to navigate to the Docker Hub page of the Ubuntu image. Once there, go through the \u0026lsquo;What is Ubuntu?\u0026rsquo;, and \u0026lsquo;What\u0026rsquo;s in this image?\u0026rsquo; section. As you might have read, this image is Ubuntu. We can use this image for general purpose stuff on Ubuntu.\nNow let\u0026rsquo;s go to the \u0026lsquo;Tags\u0026rsquo; section on the webpage, and see what we find there. We can see that there many tags, and the first one that appears should have the TAG \u0026lsquo;Latest\u0026rsquo;, and on it\u0026rsquo;s right, there should be a command that says: docker pull ubuntu:latest. If we look at the next tag after Latest, notice how the docker pull command changes slightly with the name of the TAG that we are referring to! Also, take a look at the size of the image. It should be around 25-30 MB. WHATTTTTTT!!!! This means that the docker image that can run Ubuntu instructions is only 30 MB in size.üôÄ\nLet\u0026rsquo;s stop for a moment here and summarize what we have seen so far:\nDocker Hub has many pre-built images ready for us to explore Almost all the images have a description about what they are and a small summary of what they contain. Some have much greater description in terms of the Environment Variables, but let\u0026rsquo;s look at it later All the pre-built images on Docker Hub have Tags, and every tag is a version of the image Every image tag has a set of instructions about how to pull it. Let\u0026rsquo;s understand what the pull command does!üêï\nDocker Pullüßó Seeing that we have a command linked to a image on Docker Hub, naturally the next step is to get our hands dirty. This helps me get out of my comfort zone, but also helps me bring a concept that I am trying to grasp much easier to understand. I hope this also works the same for youüêà\nImp. Noteüê≥: For getting our hands dirty here, it is advised to go through the installation of Docker Desktop. Personally, I like the Docker Engine with the compose tool more, but to begin with, either of the two installations should work fine.\nWith Docker installed on your system, let\u0026rsquo;s verify the docker version. This can be done by opening a terminal and typing:\ndocker --version If you see a message with version, then we are good to get into the fun part.\nIn the terminal, type the below command to check the current images that docker has on your machine:\ndocker images If you see nothing, or only see the hello-world image, then we are good to go. Let\u0026rsquo;s now run the command that we saw on Docker Hub page of Ubuntu:\ndocker pull ubuntu:latest Once you run this command, you should see something similar to this:\nlatest: Pulling from library/ubuntu 6e3729cf69e0: Pull complete Digest: sha256:27cb6e6ccef575a4698b66f5de06c7ecd61589132d5a91d098f7f3f9285415a9 Status: Downloaded newer image for ubuntu:latest docker.io/library/ubuntu:latest What docker pull does is it pulls the image from Docker Hub into your local machine. Once the above output is visible in the terminal, we can check the images again with docker images, and now we should see the ubuntu image with the latest tag in the terminal.\nThis is the way for us to pull pre-built images from Docker Hub\nDockerFileüê≥ Since Docker images are a union of layered filesystem stacked on top of each other, we can also build our own custom image that does something that we want by building it on top of another pre-built image. To build our own image, we first need to create a DockerFile, which is a configuration file that has commands that Docker uses to build our image. Let\u0026rsquo;s have a look at the terminology that the DockerFile has.\nTerminologyüìñ DockerFile is just a text document that contains a set of instructions in order that Docker has to execute to build our image. Let\u0026rsquo;s now look at some of the most used commands that we need to build our own Docker image, while the main list of commands is still available under DockerFile reference.\nFROM: This commands sets the base image that we want our own image to be built on top of. Every DockerFile should start with a FROM command, since every image is built on top of another image.\nRUN: As the name suggests, this command is used to execute a set of instructions on top of the base image that we use for our own custom image. RUN commands can be run in two ways: either in shell form, or in exec form, which means there are two ways to run commands for the RUN instruction. The shell form for RUN directly uses the instruction like RUN sh -c echo hello, while in the exec form, the same command would be run as RUN [\u0026quot;sh\u0026quot;, \u0026quot;-c\u0026quot;, \u0026quot;echo hello\u0026quot;]\nCOPY: This command is used to copy something from our project code inside the Docker image. The copy command needs a source location to copy from and a destination location to copy inside the Docker image.\nWORKDIR: Sets the current working directory for the Docker image, such that all the instructions that follow the after setting WORKDIR will be executed from that directory.\nCMD: This instruction is used to specify the default arguments that we want the Docker image to take when we run it. There can only be one CMD instruction in a DockerFile, and if we have multiple, then only the last one will be executed. CMD instruction works as setting default arguments when we want to run an image.\nENTRYPOINT: As CMD sets a default command, the ENTRYPOINT command can override it. However, how the command is overridden depends on the whether the shell form is used or the exec form is used. The most intuitive explanation of the interaction between CMD and ENTRYPOINT is in the Docker documentation\nWhew!!üòå That was a lot of terminology, and we have not even covered eveything from the DockerFile reference. However, we do not need to understand how each and every parameter works in order to get started. Instead, these are the most common commands that are usually inside a DockerFile while building custom images.\nLet\u0026rsquo;s now build our own Docker image by building a DockerFileü§©\nCreating a DockerFile‚úçÔ∏è If you have gone through the installation process of installing Docker, you might have come across the hello-world example of docker. But in our case, let\u0026rsquo;s redo the hello-world example in Python, but by building our own Docker image.\nFirst thing is to create a simple Python script by placing the following line inside:\nprint(f\u0026#39;Hello world from inside the Docker container!\u0026#39;) Next step is to copy the below code into a file and naming it Dockerfile:\nFROM python WORKDIR /usr/src/app COPY hello_world.py ./ CMD [\u0026#34;python\u0026#34;, \u0026#34;hello_world.py\u0026#34;] In this file, we can see the following thing:\nThe first command we write is the FROM instruction, with the python image being used. Note that if we do not mention a default tag, then the latest tag is taken as default. The second instruction is setting the WORKDIR, which is like navigating to the /usr/src/app directory inside the container. The third instruction is COPY, which selects the hello_world.py to the current working directory set by the WORKDIR command The last instruction is CMD, which sets the default command to run when the image is started as a container. Building your image‚öíÔ∏è Once the above file is created, navigate to the folder containing the file and build the image using the Dockerfile that we created in the step above. The build command is used to build an image from a particular Dockerfile by using all the instructions from inside the Dockerfile. The -t flag is used to give a name and a tag to our image, similar to the name and the tag we see on Docker Hub. Run the following command in the terminal where the Dockerfile is located:\ndocker build -t custom_image:latest . Once the above command is run, we should see something like this in the terminal:\nStep 1/4 : FROM python latest: Pulling from library/python f2f58072e9ed: Pull complete 5c8cfbf51e6e: Pull complete aa3a609d1579: Pull complete 094e7d9bb04e: Pull complete 2cbfd734f382: Pull complete aa86ac293d0f: Pull complete 4cffc9f44941: Pull complete ae2c75627c86: Pull complete 2d2b74d2f0f7: Pull complete Digest: sha256:11560799e4311fd5abcca7ace13585756d7222ce5471162cd78c78a4ecaf62bd Status: Downloaded newer image for python:latest ---\u0026gt; 539eccd5ee4e Step 2/4 : WORKDIR /usr/src/app ---\u0026gt; Running in a63f44fb58c6 Removing intermediate container a63f44fb58c6 ---\u0026gt; 266dd62fca08 Step 3/4 : COPY hello_world.py ./ ---\u0026gt; 016f934d50e5 Step 4/4 : CMD [\u0026#34;python\u0026#34;, \u0026#34;hello_world.py\u0026#34;] ---\u0026gt; Running in b38de1b256fa Removing intermediate container b38de1b256fa ---\u0026gt; 432ba341135f Successfully built 432ba341135f Successfully tagged custom_image:latest As we analyze the output of the build command, we can see the following things:\nEvery instruction starts with the Step comment, followed by the step number which is currently being executed. For example, in Step 1/4, we can see that the docker engine is pulling the image from the Docker Hub to our local machine. Every instruction that we provided in the Dockerfile is echoed first, followed by its execution hash. Once all the instructions are executed, we get the final message saying Successfully built \u0026lt;hash\u0026gt; and Successfully tagged custom_image:latest. This message means that our custom image is now built. [Bonus]Running your custom imageüèÉ Once the above commands are run in the order we specify, we can check the list of images we have on our local machine by running the following command in the terminal:\ndocker images Here, we should see our custom_image being listed amongst other images, if any. Now that we have built our custom image, it is time to run it. For now, let\u0026rsquo;s copy the following command in the terminal to run the image as a container (Don\u0026rsquo;t worry about the below line of code, we will look at it in the here):\ndocker run custom_image:latest Once we execute the above command, we should see our print message in the terminal: Hello world from inside the Docker container!\nCongratulations!!ü•≥üéâ You have successfully built your first custom image. You are already a Docker Pro!!ü§ì\nSumming Up!üí° We saw in this post how once can either pull images from the Docker Hub or create their own custom images, and run them from the terminal. The next post will now focus more on the later part of running the image once we have it on our machine.\nUntil then, Cheers!!üï∫üíÉ\n","permalink":"https://tanmaychimurkar.github.io/posts/docker/docker_images/","summary":"Docker Imagesüê≥ As we saw in the basics of Docker post, the base flow of every Docker container is an image. A Docker image, whether custom or pre-built, is absolutely necessary for us to be able to run anything using Docker.\nIn essence, a Docker image is a union of layered filesystem stacked on top of each other. This stands true for pre-built images as well as custom images that we might create using some pre-built image as a base.","title":"Docker Images and DockerFile"},{"content":"Programming a Guessing Game üé≤ In the previous chapter, we learned about the basics of Rust. In this chapter, we will learn about the basics of Rust by building a simple guessing game.\nSetting up the project We will use the Cargo package manager to create a new project called guessing_game. We can create a new project using the following command:\ncargo new guessing_game This creates a new project called guessing_game in the current directory. The Cargo package manager creates a new directory with the following structure:\nguessing_game ‚îú‚îÄ‚îÄ Cargo.toml ‚îî‚îÄ‚îÄ src ‚îî‚îÄ‚îÄ main.rs The Cargo.toml file contains the metadata of the project. The src directory contains the source code of the project. The main.rs file contains the main function of the project. As we saw in the previous chapter, the Cargo package manager uses the main.rs file as the entry point of the project. The Cargo package manager uses the Cargo.toml file to manage the dependencies of the project.\nGuessing game code The full code for the guessing game, which is in the main.rs file, looks as follows:\nuse std::io; use rand::Rng; fn main() { println!(\u0026#34;Guess the number!\u0026#34;); let secret_number = rand::thread_rng().gen_range(1..=101); loop { println!(\u0026#34;Please input your guess.\u0026#34;); let mut guess = String::new(); io::stdin() .read_line(\u0026amp;mut guess) .expect(\u0026#34;Failed to read line\u0026#34;); let guess: i32 = match guess.trim().parse() { Ok(num) =\u0026gt; num, Err(_) =\u0026gt; { println!(\u0026#34;This is not expected. Please enter an integer\u0026#34;); continue; }, }; println!(\u0026#34;You guessed: {}\u0026#34;, guess); match guess.cmp(\u0026amp;secret_number) { Ordering::Less =\u0026gt; println!(\u0026#34;Too small!\u0026#34;), Ordering::Greater =\u0026gt; println!(\u0026#34;Too big!\u0026#34;), Ordering::Equal =\u0026gt; { println!(\u0026#34;You win! You are the guessing game champion!\u0026#34;); break; }, } } } Much of the code looks like mumbo jumbo for now, but we will break it down in pieces.\nBreakdown of the above code The use keyword is used to import the io module from the standard library. The io module contains the stdin function which is used to read the input from the user. The stdin function returns an instance of the Stdin type. The Stdin type implements the Read trait. The Read trait defines the read_line method which is used to read a line from the Stdin type.\nNOTE: Traits are similar to interfaces in other languages. They define the methods that a type must implement. We will learn more about traits in a later chapter.\nPoints to remember:\nThe main function is the entry point of the program.\nThe println! macro is used to print the string to the standard output. The println! macro is similar to the printf function in C or the print function in Python. The println! macro is a macro because it is prefixed with an exclamation mark. We will learn more about macros in a later chapter.\nThe let keyword is used to create a new variable.\nThe mut keyword is used to make the variable mutable.\nThe String::new function is used to create a new empty string. The String type is a string type provided by the standard library. The String type is a growable, UTF-8 encoded string.\nHowever, we still need a way to read the input from the user. The read_line method takes the input from the user and stores it in the guess variable. The read_line method takes the input as a mutable reference.\nThe \u0026amp; symbol is used to create a reference. The \u0026amp;mut symbol is used to create a mutable reference.\nThe read_line method returns a Result type. The Result type is an enum which has two variants: Ok and Err. The Ok variant indicates that the operation was successful. The Err variant indicates that the operation failed. The expect method is used to handle the Err variant. The expect method takes a string as an argument. If the Result type is Ok, the expect method returns the value inside the Ok variant. If the Result type is Err, the expect method prints the string passed to it and exits the program. If we don\u0026rsquo;t call the expect method, the program will compile but will throw a warning.\n$ cargo build Compiling guessing_game v0.1.0 (file:///projects/guessing_game) warning: unused `Result` that must be used --\u0026gt; src/main.rs:10:5 | 10 | io::stdin().read_line(\u0026amp;mut guess); | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ | = note: `#[warn(unused_must_use)]` on by default = note: this `Result` may be an `Err` variant, which should be handled warning: `guessing_game` (bin \u0026#34;guessing_game\u0026#34;) generated 1 warning Finished dev [unoptimized + debuginfo] target(s) in 0.59s The Rust compiler is smart enough to detect that the Result type returned by the read_line method is not being used. The Rust compiler throws a warning to let us know that we are not handling the Err variant of the Result type.\nPhew! That was a lot of information. Take a look at the code again and make sure we can understand the code with the new information that we have learned.\nBuilding and Running the code We can run the code using the following command:\ncargo run Key takeaways:\ncargo run will build and run the code, while cargo build will only build an executable in the target/debug directory. cargo run is useful when we are developing the code. cargo run will compile the code and run the executable every time we make a change to the code. cargo build is useful when we are deploying the code. cargo build will compile the code and create an executable. Once run, the code will print the following output:\n$ cargo run Compiling guessing_game v0.1.0 (file:///projects/guessing_game) Finished dev [unoptimized + debuginfo] target(s) in 0.59s Running `target/debug/guessing_game` Guess the number! Please input your guess. 5 You guessed: 5 We can see that we can now take an input from the user. However, we still need to generate a random number with which the comparison for the user input has to be done. This random number generator functionality can be implemented smoothly by using cargo.\nGenerating a random number With cargo, we have the option to get crates. Crates are packages of Rust code that we can use in our project. We can get a crate by adding the crate name and the version number to the Cargo.toml file. The Cargo package manager will download the crate and add it to the Cargo.lock\nFor our case, we need to add the rand crate to the Cargo.toml file. The rand crate is a crate that provides random number generation functionality. To add the rand crate to the Cargo.toml file, we need to add the following line to the Cargo.toml file:\n[dependencies] rand = \u0026#34;0.8.5\u0026#34; The documentation of the rand crate can be found here.\nIf you build the project now with cargo build, we should see that the rand create is being fetched and added to the Cargo.lock file.\n$ cargo build Updating crates.io index Downloading crates ... Downloaded rand v0.8.5 Downloaded 1 crate (1.1 MB) in 0.75s Compiling rand v0.8.5 Compiling guessing_game v0.1.0 (file:///projects/guessing_game) Finished dev [unoptimized + debuginfo] target(s) in 1.88s However, if you run it again, we will not see the above output. cargo will check the Cargo.lock file to see if the dependencies have changed. If the dependencies have not changed, cargo will not fetch the dependencies again. This is useful when we are deploying the code, since we can be sure that the dependencies will not change when we deploy the code, and we will deploy the code with the same dependencies that we were using while developing the code.\nGenerating Random Number for our game Now that we have the rand crate, we can use it to generate a random number. We can use the gen_range method of the rand crate to generate a random number. The gen_range method takes two arguments: the lower bound and the upper bound. The gen_range method will generate a random number between the lower bound and the upper bound.\nuse std::io; use rand::Rng; fn main() { println!(\u0026#34;Guess the number!\u0026#34;); let secret_number = rand::thread_rng().gen_range(1..=101); println!(\u0026#34;The secret number is: {}\u0026#34;, secret_number); println!(\u0026#34;Please input your guess.\u0026#34;); let mut guess = String::new(); io::stdin() .read_line(\u0026amp;mut guess) .expect(\u0026#34;Failed to read line\u0026#34;); println!(\u0026#34;You guessed: {}\u0026#34;, guess); } If you run the code now, you will see that the code will generate a random number between 1 and 101, and will print it to the console.\n$ cargo run Compiling guessing_game v0.1.0 (file:///projects/guessing_game) Finished dev [unoptimized + debuginfo] target(s) in 1.02s Running `target/debug/guessing_game` Guess the number! The secret number is: 42 Please input your guess. 5 You guessed: 5 Now we see that a random number is being generated. However, we still need to compare the user input with the random number. We will do this in the next section.\nComparing the user input with the random number We can compare the user input with the random number using the cmp method.\nPoints to remeber:\nThe cmp method takes a reference to the value that we want to compare with. The cmp method returns an Ordering type. The Ordering type is an enum that can have three values: Less, Greater, and Equal. The cmp method compares the value that we are calling the method on with the value that we pass as an argument to the cmp method. If the value that we are calling the method on is less than the value that we pass as an argument to the cmp method, the cmp method will return the Less variant of the Ordering type. If the value that we are calling the method on is greater than the value that we pass as an argument to the cmp method, the cmp method will return the Greater variant of the Ordering type. If the value that we are calling the method on is equal to the value that we pass as an argument to the cmp method, the cmp method will return the Equal variant of the Ordering type. We can use the match expression to handle the Ordering type returned by the cmp method. The match expression is similar to the switch statement in other languages. The match expression takes a value as an argument and compares the value with the patterns that we specify in the match expression. If the value matches the pattern, the code that is associated with the pattern will be executed. If the value does not match any of the patterns, the match expression will throw an error. use std::io; use rand::Rng; fn main() { println!(\u0026#34;Guess the number!\u0026#34;); let secret_number = rand::thread_rng().gen_range(1..=101); println!(\u0026#34;The secret number is: {}\u0026#34;, secret_number); println!(\u0026#34;Please input your guess.\u0026#34;); let mut guess = String::new(); io::stdin() .read_line(\u0026amp;mut guess) .expect(\u0026#34;Failed to read line\u0026#34;); println!(\u0026#34;You guessed: {}\u0026#34;, guess); match guess.cmp(\u0026amp;secret_number) { Ordering::Less =\u0026gt; println!(\u0026#34;Too small!\u0026#34;), Ordering::Greater =\u0026gt; println!(\u0026#34;Too big!\u0026#34;), Ordering::Equal =\u0026gt; println!(\u0026#34;You win! You are the guessing game champion!\u0026#34;), } } If you run the code now, you will see that the code will not compile.\nPoints to remember: The reason for this is that we are trying to compare a String type with an i32 type. We can fix this by converting the String type to an i32 type. We can do this by using the trim method to remove the newline character from the String type, and then using the parse method to convert the String type to an i32 type.\nuse std::io; use rand::Rng; fn main() { println!(\u0026#34;Guess the number!\u0026#34;); let secret_number = rand::thread_rng().gen_range(1..=101); println!(\u0026#34;The secret number is: {}\u0026#34;, secret_number); println!(\u0026#34;Please input your guess.\u0026#34;); let mut guess = String::new(); io::stdin() .read_line(\u0026amp;mut guess) .expect(\u0026#34;Failed to read line\u0026#34;); let guess: i32 = guess.trim().parse().expect(\u0026#34;Please type a number!\u0026#34;); println!(\u0026#34;You guessed: {}\u0026#34;, guess); match guess.cmp(\u0026amp;secret_number) { Ordering::Less =\u0026gt; println!(\u0026#34;Too small!\u0026#34;), Ordering::Greater =\u0026gt; println!(\u0026#34;Too big!\u0026#34;), Ordering::Equal =\u0026gt; println!(\u0026#34;You win! You are the guessing game champion!\u0026#34;), } } If you run the code now, you will see that the code will compile and run.\nHowever, we can still type an alphabet and get away with it. We can fix the input to be only numbers by using the Result type returned by the parse method. The Result is of type enum and can have two values: Ok and Err.\nPoints to remember:\nThe Ok variant of the Result type means that the operation was successful The Err variant of the Result type means that the operation failed. The parse method will return the Ok variant of the Result type if the conversion was successful, and will return the Err variant of the Result type if the conversion failed. We can use the match expression to handle the Result type returned by the parse method. If the parse method returns the Ok variant of the Result type, we will assign the value that is inside the Ok variant to the guess variable. If the parse method returns the Err variant of the Result type, we will print the error message that is inside the Err variant to the console. With these points, we can change the code as follows:\nuse std::io; use rand::Rng; fn main() { println!(\u0026#34;Guess the number!\u0026#34;); let secret_number = rand::thread_rng().gen_range(1..=101); println!(\u0026#34;The secret number is: {}\u0026#34;, secret_number); println!(\u0026#34;Please input your guess.\u0026#34;); let mut guess = String::new(); io::stdin() .read_line(\u0026amp;mut guess) .expect(\u0026#34;Failed to read line\u0026#34;); let guess: i32 = match guess.trim().parse() { Ok(num) =\u0026gt; num, Err(_) =\u0026gt; { println!(\u0026#34;This is not expected. Please enter an integer\u0026#34;); continue; }, }; println!(\u0026#34;You guessed: {}\u0026#34;, guess); match guess.cmp(\u0026amp;secret_number) { Ordering::Less =\u0026gt; println!(\u0026#34;Too small!\u0026#34;), Ordering::Greater =\u0026gt; println!(\u0026#34;Too big!\u0026#34;), Ordering::Equal =\u0026gt; println!(\u0026#34;You win! You are the guessing game champion!\u0026#34;), } } If you run the code now, you will see that the code will compile and run. However, if you enter a non-numeric value, the code will not panic. Instead, the code will print the error message that we specified in the Err variant of the Result type. We handled the panic using the Err variant of the Result type.\nLooping until correct number is guessed We can use loop to keep the program running until the correct number is guessed. This can be done as follows:\nuse std::io; use rand::Rng; fn main() { println!(\u0026#34;Guess the number!\u0026#34;); let secret_number = rand::thread_rng().gen_range(1..=101); println!(\u0026#34;The secret number is: {}\u0026#34;, secret_number); loop { println!(\u0026#34;Please input your guess.\u0026#34;); let mut guess = String::new(); io::stdin() .read_line(\u0026amp;mut guess) .expect(\u0026#34;Failed to read line\u0026#34;); let guess: i32 = match guess.trim().parse() { Ok(num) =\u0026gt; num, Err(_) =\u0026gt; { println!(\u0026#34;This is not expected. Please enter an integer\u0026#34;); continue; }, }; println!(\u0026#34;You guessed: {}\u0026#34;, guess); match guess.cmp(\u0026amp;secret_number) { Ordering::Less =\u0026gt; println!(\u0026#34;Too small!\u0026#34;), Ordering::Greater =\u0026gt; println!(\u0026#34;Too big!\u0026#34;), Ordering::Equal =\u0026gt; { println!(\u0026#34;You win! You are the guessing game champion!\u0026#34;); break; }, } } } Points to remember:\nWe just use loop keyword to create an infinite loop. We can use break keyword to break out of the loop. We can also use continue keyword to skip the rest of the loop and start the next iteration of the loop.\nRemoving the secret number message We just have to remove the secret number generation part from the code. Once we remove the secret number generation part from the code, we will not be able to print the secret number to the console.\nThe final script should then look like this:\nuse std::io; use rand::Rng; fn main() { println!(\u0026#34;Guess the number!\u0026#34;); let secret_number = rand::thread_rng().gen_range(1..=101); loop { println!(\u0026#34;Please input your guess.\u0026#34;); let mut guess = String::new(); io::stdin() .read_line(\u0026amp;mut guess) .expect(\u0026#34;Failed to read line\u0026#34;); let guess: i32 = match guess.trim().parse() { Ok(num) =\u0026gt; num, Err(_) =\u0026gt; { println!(\u0026#34;This is not expected. Please enter an integer\u0026#34;); continue; }, }; println!(\u0026#34;You guessed: {}\u0026#34;, guess); match guess.cmp(\u0026amp;secret_number) { Ordering::Less =\u0026gt; println!(\u0026#34;Too small!\u0026#34;), Ordering::Greater =\u0026gt; println!(\u0026#34;Too big!\u0026#34;), Ordering::Equal =\u0026gt; { println!(\u0026#34;You win! You are the guessing game champion!\u0026#34;); break; }, } } } ","permalink":"https://tanmaychimurkar.github.io/posts/rust/chap2/","summary":"Programming a Guessing Game üé≤ In the previous chapter, we learned about the basics of Rust. In this chapter, we will learn about the basics of Rust by building a simple guessing game.\nSetting up the project We will use the Cargo package manager to create a new project called guessing_game. We can create a new project using the following command:\ncargo new guessing_game This creates a new project called guessing_game in the current directory.","title":"Rust Documentation: Chapter 2"},{"content":"What is Rust? Rust is a programming language that is designed to be safe, fast, and concurrent. It is a systems programming language that is used to build low-level software. It is a statically typed language that is compiled to machine code. It is a language that is used to build operating systems, browsers, and other low-level software.\nThis blog post summarizes the key takeaways about the origins of Rust and its usefulness in software development MIT Technology Review: How Rust went from a side project to the world‚Äôs most-loved programming language\nWhy Rust? There are several reasons why one might choose to use Rust as their programming language:\nPerformance: Rust is a systems programming language that is designed to be fast and efficient. Its focus on memory safety and low-level control makes it well-suited for developing high-performance applications.\nMemory safety: Rust\u0026rsquo;s ownership and borrowing model allows for strict compile-time checks that prevent common memory-related bugs such as null pointer dereferences, buffer overflows, and use-after-free errors. This can significantly reduce the occurrence of runtime crashes and security vulnerabilities.\nConcurrency: Rust has built-in support for concurrency and parallelism, making it well-suited for developing scalable and efficient applications.\nCommunity: Rust has a growing and active community that provides a wealth of libraries, tools, and resources for developers. This community is committed to open-source development and creating a safe and inclusive space for programmers.\nCross-platform support: Rust supports a wide range of platforms, including Linux, macOS, Windows, and many embedded systems. This makes it a versatile choice for developing applications that need to run on different platforms.\nOverall, Rust is a modern programming language that offers a unique combination of performance, safety, concurrency, and community support. It\u0026rsquo;s an excellent choice for developing systems-level software, performance-critical applications, and any project that requires both speed and safety.\nWhat is Rust used for? Rust is a systems programming language that is well-suited for developing high-performance, concurrent, and safe applications. Here are some of the common use cases for Rust:\nSystems programming: Rust is often used for developing operating systems, device drivers, file systems, and other low-level software that require direct access to hardware.\nWeb development: Rust is increasingly being used for web development, particularly for building back-end services and APIs. Rust\u0026rsquo;s strong typing, memory safety, and concurrency features make it a good fit for developing fast and reliable web applications.\nGame development: Rust\u0026rsquo;s performance and memory safety features make it well-suited for developing high-performance game engines and libraries.\nNetwork programming: Rust\u0026rsquo;s support for concurrency and asynchronous I/O make it a good choice for developing network services, such as web servers, proxies, and load balancers.\nData processing: Rust\u0026rsquo;s performance and memory safety features make it well-suited for developing data processing and analytics applications, such as data pipelines and machine learning frameworks.\nEmbedded systems: Rust\u0026rsquo;s low-level control, memory safety, and cross-platform support make it a good choice for developing software for embedded systems, such as microcontrollers and IoT devices.\nOverall, Rust\u0026rsquo;s performance, safety, and versatility make it a good choice for developing a wide range of applications, particularly those that require high performance, concurrency, and memory safety.\nInstalling Rust ü¶Ä For Linux based systems, we can install Rust using the following command:\ncurl --proto \u0026#39;=https\u0026#39; --tlsv1.2 https://sh.rustup.rs -sSf | sh This installs the rustup toolchain manager, which installs the latest stable version of Rust by default. The Cargo package manager is also installed by default. Installation for other operating systems can be found here.\nWe can update the rustup toolchain manager using the following command:\nrustup update Hello, World! from ü¶Ä As it is tradition, we will start with the Hello, World! program in Rust.\nBefore starting, we first need to create a new project using the following command:\ncargo new hello_world This creates a new project called hello_world in the current directory. The Cargo package manager is used to manage Rust projects. The Cargo.toml file contains the project metadata and dependencies. The src directory contains the source code for the project.\nmain.rs in src The main.rs file in the src directory contains the source code for the hello_world project. The main function is the entry point for the program. The println! macro is used to print the Hello, World! string to the console.\nfn main() { println!(\u0026#34;Hello, world!\u0026#34;); } Compiling and running the program We can compile the program using the following command:\ncargo build This compiles the program and generates the executable file in the target/debug directory. We can run the program using the following command:\ncargo run This compiles and runs the program. We can also run the program using the following command:\n./target/debug/hello_world Cargo üì¶ Cargo is the Rust package manager and build tool. It is used to manage Rust projects and their dependencies. It is used to build, test, and run Rust programs. It is used to create new Rust projects and add dependencies to existing projects.\nThe Chapter 1 of Rust documentation can be found here.\n","permalink":"https://tanmaychimurkar.github.io/posts/rust/chap1/","summary":"What is Rust? Rust is a programming language that is designed to be safe, fast, and concurrent. It is a systems programming language that is used to build low-level software. It is a statically typed language that is compiled to machine code. It is a language that is used to build operating systems, browsers, and other low-level software.\nThis blog post summarizes the key takeaways about the origins of Rust and its usefulness in software development MIT Technology Review: How Rust went from a side project to the world‚Äôs most-loved programming language","title":"Rust Documentation: Chapter 1"},{"content":"Why Tmux‚ÅâÔ∏è Every software engineer, or people working with software development, have had some sort of interaction with a terminal. Now, terminals are great! If one really masters it, there is no need to use your default OS file explorer to get around files that you need.\nHowever, I personally find it quite chaotic to use many many many terminal windows when trying to work on a single project. The problem of now knowing which terminal window has what can be mind-boggling, and if you have even 5 of such windows, you are already at a trouble to know which terminal has what process running.\nEnter Tmux\nWhat is Tmux‚ùì Tmux is a terminal multiplexer for Ubuntu (and other Linux-based operating systems). It allows users to run multiple terminal sessions within a single terminal window and switch between them easily. Tmux also allows users to detach and reattach sessions, which makes it useful for running long-running commands or keeping a session open even when you are not actively using it.\nIn short, we can split a single terminal into many terminal windows, without having to open many terminal windows for each separate tasks. We can also name our terminal sessions in Tmux, which makes it easier to keep track of which terminal window is for which job. The detach and reattach functions are also super convenient, since we can temporarily suspend our terminals and resume back to them whenever we want.\nSo, let\u0026rsquo;s now see how one would go about using Tmux!!\nInstalling Tmux To install Tmux on Ubuntu, you can use the following command in your terminal:\nsudo apt-get install tmux Once finished, you should have Tmux installed on your machine. Once installed, you can start Tmux by running the command Tmux in the terminal. Once you are in Tmux, you can use various key commands to navigate and manage your sessions, windows, and panes. You can find more information on how to use Tmux by running man Tmux in the terminal. However, I can help you get started with a few useful things to move around tmux.\nTmux shortcuts Create Splits Once you type tmux in your favourite terminal, you should see the terminal on the default directory of your machine. From here, type in your first command using:\n: ctrl + b % When you type the colon :, look at the bottom of the window to see the prompt changing to :. The ctrl + b is the default prefix that Tmux has when you want to execute Tmux commands in a window. The % symbol is used to split the terminal horizontally. Similarly, replacing % with ' will split the window vertically.\nNavigating Panes Now that you have your first split, start typing your commands in any of the splits. To switch between the terminal windows, there are two ways:\nprefix + \u0026lt;direction\u0026gt;: Here prefix is the default prefix Ctrl + b and direction is one of the arrow keys (up, down, left, or right) to move the focus to the corresponding pane. prefix + o: To move the focus to the next pane in the current window. prefix + { or prefix + } : To move the focus to the next or previous pane in the current window prefix + q + \u0026lt;pane-number\u0026gt;: Prompts the number of the panes on screen, and you choose the number where you want to switch. Resize Windows We can also resize our splits that we created with the following commands:\nprefix + \u0026lt;direction\u0026gt; + \u0026lt;arrow key\u0026gt;: to resize the current pane in the specified direction. prefix + \u0026lt;direction\u0026gt; + \u0026lt;arrow key\u0026gt; + \u0026lt;Shift\u0026gt;: to resize the current pane more quickly in the specified direction. Tmux configuration file The default prefix that Tmux provides is a bit unnatural to many programmers, and this can be fixed very easily. To fix this, you just need to create a tmux.conf file in the root system, and add the following line there:\nset-hook -g after-new-session \u0026#34;source-file ~/.tmux.conf\u0026#34; unbind-key C-b set-option -g prefix C-\u0026lt;enter new prefix key here\u0026gt; bind-key C-g send-prefix Once you enter your favourite prefix key in the set-option command, you will have changed your prefix. For the changes to appear, either restart tmux server, or type the following in any of the tmux panes:\n: source ~/.tmux.conf This will source your changes from the config file, and your prefix should be changed.\nSome popular changes to replace default behaviour in Tmux Generally, these are the configuration files that I have seen programmers use quite often:\nset-hook -g after-new-session \u0026#34;source-file ~/.tmux.conf\u0026#34; unbind-key C-b set-option -g prefix C-g bind-key C-g send-prefix set-window-option -g mode-keys vi bind e setw synchronize-panes on\\; display-message \u0026#34;Panes are synchronized\u0026#34; bind E setw synchronize-panes off\\; display-message \u0026#34;Panes not synchronized\u0026#34; bind h select-pane -L bind j select-pane -D bind k select-pane -U bind l select-pane -R To know what each of them do, type the prefix key that you set, and then the alphabet following the bind keyword in the above configuration.\nThis can be used as a good starting point for your tmux.conf file.\nCongratulations You are now a pro terminal user who uses tmux to set up their workflow. Hopefully, you should soon see a boost in your productivity, and will also fall in love with your terminal :)\nUntil next time!!\n","permalink":"https://tanmaychimurkar.github.io/posts/tools/tmux/","summary":"Why Tmux‚ÅâÔ∏è Every software engineer, or people working with software development, have had some sort of interaction with a terminal. Now, terminals are great! If one really masters it, there is no need to use your default OS file explorer to get around files that you need.\nHowever, I personally find it quite chaotic to use many many many terminal windows when trying to work on a single project. The problem of now knowing which terminal window has what can be mind-boggling, and if you have even 5 of such windows, you are already at a trouble to know which terminal has what process running.","title":"Tmux-Ease your terminal workflow"},{"content":"Why Docker Compose‚ÅâÔ∏è In the previous post, we saw how to run a Docker Container from a Docker image, either custom or pre-built image from the DockerHub. However, as we discussed, a big software is not built on just one Docker Container but many containers, working as microservices. When running multiple such containers, it is important that they are able to interact with each other, be able to exchange data, and thus act as independent blocks of a larger piece of software. In this post, we will go over how to thus run multiple containers together.\nWhat is Docker Compose file?üêô Running multiple containers can be done by a Docker Compose file, that acts as a configuration file to choose multiple Docker Images and start them as containers, such that they are able to interact with each other.\nDocker Compose file, also known as docker-compose.yml, is a configuration file used to define the services, networks and volumes for a multi-container Docker application. With the help of docker-compose command, the services defined in the file can be easily started, stopped, scaled, and managed as a single unit. The file is written in YAML format, which is easy to read and understand. It allows developers to define the complete environment for an application in a single file, and manage it more efficiently.\nDon\u0026rsquo;t be fooled by me throwing the terms services, networks and volumes from the description as some sort of concepts that everyone must know. These are terminologies specific to the docker-compose.yml file, and are used as a standard base to create the compose configuration file. Let\u0026rsquo;s look at them in a short detail below.\ndocker-compose.yml terminologyüìö Below is a list of some of the main terminology that a docker-compose.yml file should follow.\nservices: The services section is used to define the individual components or microservices that make up the application. Each service is defined as a separate block, with its own configuration options. The options available for each service include the image to use, environment variables, ports to expose, volumes to mount, and links to other services. For example, a simple web application might have two services: a web server and a database. The web server service would be defined with the image to use, environment variables, and ports to expose. The database service would be defined with the image to use, environment variables, and volumes to mount.\nnetworks: The networks section is used to define virtual networks that the services in the application can connect to. Services can be connected to multiple networks, allowing them to communicate with each other. When a service is connected to a network, it is given an IP address on that network and can communicate with other services on the same network using their IP addresses. This allows services to be isolated from the host system and from the external network, while still being able to communicate with each other. It\u0026rsquo;s also possible to create custom networks and connect services to them. For example, a docker-compose.yml file may define a default network called default and a custom network called backend where all services that need to communicate with each other privately are connected to.\nvolumes: The volumes section is used to define storage volumes that can be used by the services in the application. A volume is a way to store data outside a container\u0026rsquo;s filesystem, so that it can be accessed by multiple containers and persist data even if the container is deleted. The storage volumes defined by us are stored in our local machine, under the /var/lib/docker/volumes folder. Volumes can be defined in a docker-compose.yml file and then mounted to a specific service\u0026rsquo;s container. They can also be defined as external, which means that they are managed outside docker-compose and can be shared by multiple applications. For example, if you have a service that needs to store some data, you can create a volume and mount it to that service\u0026rsquo;s container. This way, even if the container is recreated or deleted, the data stored in the volume will persist. It\u0026rsquo;s also possible to use named volumes, which allows you to reference the volume by name instead of by path on the host. This can make it easier to manage volumes across different environments.\nThese are only some of the standard terms that are in the docker-compose file. However, let us now look at an example of the docker-compose file itself to see what the structure of it is.\nStructure of the docker-compose.yml fileüìã As we read before, the docker-compose.yml is only a YAML configuration file. Below is the structure of a very small compose file, that runs the PostgreSQL and the pgAdmin UI tool together.\nThe below docker-compose.yml file is designed in such a way that we use most of the possible terms that appear in the docker-compose.yml file. Do not worry if you don\u0026rsquo;t completely understand each and every term in the file, we will go through their descriptions later. For now, let us have a look at how the file itself looks first.\nNOTE: If you want to learn more about what YAML files are, have a look at YAML or read the official YAML documentation.\nversion: \u0026#34;3\u0026#34; services: postgres: image: postgres restart: always environment: POSTGRES_PASSWORD: postgres POSTGRES_USER: postgres ports: - 5432:5432 volumes: - postgres:/var/lib/postgresql/data pgadmin: image: dpage/pgadmin4 environment: PGADMIN_DEFAULT_EMAIL: admin@pgadmin.com PGADMIN_DEFAULT_PASSWORD: password PGADMIN_LISTEN_PORT: 80 ports: - 1542:80 volumes: - pgadmin:/var/lib/pgadmin depends_on: - postgres volumes: postgres: pgadmin: We can now see that the terms services and volumes are defined in the above compose file, however there are also new terms that we did not yet see. Let us now go through the compose file above to see what each entry means.\npostgres/pgadmin: This is the name we want to give to the services before we start defining them. Once we run the containers defined in the compose file, we will see each running container for a service by this name. And just like in the previous post, we can exec into each of the services by the name that we choose to give it. Please note that the name is given by us, and is not something that is attached to a particular image. image: This is the image that we want the service to use when we run it. This can be either an image from the DockerHub, or our own image. If we are using owr own custom image in the compose file, then it\u0026rsquo;s relative path compared to the docker-compose.yml file should be passed in the image term. environment: This term defines a list of environment variables that we want our services to use. The environment variables are not shared across different services, and should be defined individually for each of the service that we define in the docker-compose file. ports: This term is central for the end user running the docker-compose file. This term defines the port that is to be exposed on the local machine from inside the docker network that is defined when the docker-compose.yml file is run. For us to be able to access a running application, we define the port in the first part of the ports, and the second part is the port that the service is running on inside the docker network. volumes: There are 2 ways volumes can be defined in the docker-compose file: named volumes: These volumes are managed by docker-compose and can be referenced by name in the docker-compose.yml file. bind mounts: These volumes are defined by a specific path on the host machine and mounted inside the container. Usually, it is advised to use named volumes instead of bind volumes in the compose file, as named volumes are managed directly by Docker, and can be moved/deleted using the Docker CLI. The volume that is defined in the above compose file are also named volumes, where the first half of the volumes term is the name we want to give to the names volume, and the second half is the actual location of the directory that we want to be stored in our named volume. depends_on: This is a term that controls if a service is dependent on any other service. We have to define the names of the services that we want to start before the current service where the depends_on term is defined. This makes sense by looking at the above scenario: We do not want the pgadmin service to start before the postgres service, as there will be no database to attach to, and thus the tool will not be able to locate our database. Phew!! That was a very long terminology that is followed inside the docker-compose.yml file. However, there should still be some alarm bells ringing in your head if you read the above descriptions.\nHow do we know what volume to mount on our local machine? How do we know what environment variables to define for a service to be able to run successfully? What is the default port that are particular service starts at when we run it? Fret not! These questions all have a answer that is easy to find.\nGet variables for a service based on the imageüîç All the above questions are dependent on the Docker Image that a particular service is using. For example, if you look at the Environment Variables section of the postgres image on DockerHub, then you will find that the environment variable POSTGRES_PASSWORD is the only required variable for the container to run successfully. We also pass the POSTGRES_USER variable just so we see how to pass other environment variables to a particular service as well.\nAnd from the PG_DATA section on the webpage, we can find that default directory where postgres stores the data is /var/lib/postgresql/data directory, and that is the directory that we choose to mount in a named volume in our compose file.\nNow that we have MOST of the terminology that a docker-compose.yml file has, it is time to run the compose file to see things in action.\nIMPORTANT: Before running the next part, be sure to have Docker Compose installed. Please follow this link to install the Compose tool on your machine.\nRunning your containersüì¶ Run the below command in your terminal to start all the services from the docker-compose.yml file.\ndocker-compose up NOTE: Depending on which tool you install, you should either have the docker-compose up command or the docker compose up command.\nOnce you run the command, you should see something like this in your terminal window:\n\u0026gt; docker-compose up \u0026gt; docker-compose up Creating network \u0026#34;user_default\u0026#34; with the default driver Pulling postgres (postgres:)... latest: Pulling from library/postgres 8740c948ffd4: Pull complete c8dbd2beab50: Pull complete 05d9dc9d0fbd: Pull complete ddd89d5ec714: Pull complete f98bb9f03867: Pull complete 0554611e703f: Pull complete 64e0a8694477: Pull complete 8b868a753f47: Pull complete 12ed9aefbab3: Pull complete 825b08d51ffd: Pull complete 8f272b487267: Pull complete ba2eed7bd2cc: Pull complete ff59f63f47d6: Pull complete Digest: sha256:6b07fc4fbcf551ea4546093e90cecefc9dc60d7ea8c56a4ace704940b6d6b7a3 Status: Downloaded newer image for postgres:latest Pulling pgadmin (dpage/pgadmin4:)... latest: Pulling from dpage/pgadmin4 8921db27df28: Pull complete d10ee54273de: Pull complete 3cf1e77a6858: Pull complete 07b97201e1e9: Pull complete b77bae207213: Pull complete 0fcc0c06a94f: Pull complete 3c9a847b1b09: Pull complete 6ad9bb3cc48b: Pull complete 246134c219b2: Pull complete ac0085153d3a: Pull complete 8860f79c6eae: Pull complete 8b0e5eb7caab: Pull complete 2387bc6168f4: Pull complete 0be474dc7144: Pull complete Digest: sha256:79b2d8da14e537129c28469035524a9be7cfe9107764cc96781a166c8374da1f Status: Downloaded newer image for postgres:latest Status: Downloaded newer image for dpage/pgadmin4:latest Creating user_postgres_1 ... done # Creating Users Creating user_pgadmin_1 ... done Attaching to user_postgres_1, user_pgadmin_1 # Starting services postgres_1 | postgres_1 | PostgreSQL Database directory appears to contain a database; Skipping initialization postgres_1 | postgres_1 | 2023-01-21 23:47:11.847 UTC [1] LOG: starting PostgreSQL 15.1 (Debian 15.1-1.pgdg110+1) on x86_64-pc-linux-gnu, compiled by gcc (Debian 10.2.1-6) 10.2.1 20210110, 64-bit postgres_1 | 2023-01-21 23:47:11.847 UTC [1] LOG: listening on IPv4 address \u0026#34;0.0.0.0\u0026#34;, port 5432 postgres_1 | 2023-01-21 23:47:11.847 UTC [1] LOG: listening on IPv6 address \u0026#34;::\u0026#34;, port 5432 postgres_1 | 2023-01-21 23:47:12.035 UTC [1] LOG: listening on Unix socket \u0026#34;/var/run/postgresql/.s.PGSQL.5432\u0026#34; postgres_1 | 2023-01-21 23:47:12.233 UTC [29] LOG: database system was interrupted; last known up at 2023-01-17 13:43:30 UTC postgres_1 | 2023-01-21 23:47:14.213 UTC [29] LOG: database system was not properly shut down; automatic recovery in progress postgres_1 | 2023-01-21 23:47:14.315 UTC [29] LOG: redo starts at 0/249F8F8 postgres_1 | 2023-01-21 23:47:14.315 UTC [29] LOG: invalid record length at 0/249F9E0: wanted 24, got 0 postgres_1 | 2023-01-21 23:47:14.315 UTC [29] LOG: redo done at 0/249F9A8 system usage: CPU: user: 0.00 s, system: 0.00 s, elapsed: 0.00 s postgres_1 | 2023-01-21 23:47:14.469 UTC [27] LOG: checkpoint starting: end-of-recovery immediate wait postgres_1 | 2023-01-21 23:47:14.948 UTC [27] LOG: checkpoint complete: wrote 3 buffers (0.0%); 0 WAL file(s) added, 0 removed, 0 recycled; write=0.149 s, sync=0.049 s, total=0.564 s; sync files=2, longest=0.025 s, average=0.025 s; distance=0 kB, estimate=0 kB postgres_1 | 2023-01-21 23:47:14.999 UTC [1] LOG: database system is ready to accept connections # Now pgAdmin starts pgadmin_1 | [2023-01-21 23:47:19 +0000] [1] [INFO] Starting gunicorn 20.1.0 pgadmin_1 | [2023-01-21 23:47:19 +0000] [1] [INFO] Listening at: http://[::]:80 (1) pgadmin_1 | [2023-01-21 23:47:19 +0000] [1] [INFO] Using worker: gthread pgadmin_1 | [2023-01-21 23:47:19 +0000] [86] [INFO] Booting worker with pid: 86 From the above output of running the docker-compose.yml file, we see note the following:\nThe images are first pulled, which is what we saw in the Docker Images post, Once they are pulled, we see the there are users created that we had defined in the compose file above. However, you should see at # Creating Users line in the description above to see that the user names have the prefix _1 attached to them. This is the default behaviour by Docker, and we can have our own prefix by passing the argument -p OUR_PREFIX_NAME. On the comment # Starting Services in the above output, we see that the services themselves have started. We see first logs from the postgres service, and then from the pgadmin service. Notice that the pgadmin service only starts after the # Now pgAdmin starts comment in the above output. This is inline with the depends_on clause that we had defined in our compose file. Once you see the above output, you can go on localhost:1542 (or whichever port you had chosen to be exposed on your local machine), and you should see the pgAdmin default webpage. Once there, login with the email and the password defined in the PGADMIN_DEFAULT_EMAIL and the PGADMIN_DEFAULT_PASSWORD environment variables. You should then be logged in, and should then see that the default postgres database is also visible.\nCongratulationsüôåüéâü•≥üôåüéâü•≥ Well Well Well!! Congratulations to you on being a pro Docker user and on coming this far.\nYou are now equipped with the most awesome and popular microservice creation tool in your skills bag. With this skill, you are now unstoppable in creating the largest piece of software by dividing it into many smaller parts, that are much easier to manage than a giant Monolith of code.\nThank you for reading this far, and I hope I was able to help you learn something new!! Until next time üòéüòéüòé\n","permalink":"https://tanmaychimurkar.github.io/posts/docker/docker_compose/","summary":"Why Docker Compose‚ÅâÔ∏è In the previous post, we saw how to run a Docker Container from a Docker image, either custom or pre-built image from the DockerHub. However, as we discussed, a big software is not built on just one Docker Container but many containers, working as microservices. When running multiple such containers, it is important that they are able to interact with each other, be able to exchange data, and thus act as independent blocks of a larger piece of software.","title":"Docker Compose"},{"content":"What is a Docker Container?üì¶ In the previous post, we saw how to find Docker images form the DockerHub, and also how to create our own Docker image that executes a function. In this post, we will go more over the execution part of Docker, mainly about how to run a Docker image as a container.\nDocker Containers are Docker images that become containers at run time. Docker containers are short pieces of software that runs an application quickly, as a lightweight standalone package. The containers need an engine to run, and this is the Docker Engine.\nBasically, Docker containers make it easier to decentralize a large piece of software such that it can run in isolated environments, and they are what we can refer to as microservices. So in this part of the post, we will be running our first use case of microservices. We will start with running a single Docker image first, then we will combine many Docker images to run together so that they can interact with each other and run the whole software.\nRunning a ContainerüèÉ In the previous post, we saw how we can either pull a pre-built Docker image from the DockerHub, or create our own custom image. We saw at the end that we had to run a specific command in the terminal to get the output from a Docker image. The commands that we ran in the previous post ran the Docker image, and during runtime, it created a Docker container that runs the piece of code that we put inside our own image or the outputs from a pre-built image from the DockerHub.\nIf you followed along with the previous post, you should now see the Ubuntu image in the terminal when you execute the below command in your terminal.\n\u0026gt; docker images REPOSITORY TAG IMAGE ID CREATED SIZE custom_image latest 432ba341135f 4 weeks ago 932MB ubuntu latest 6b7dfa7e8fdb 5 weeks ago 77.8MB In case you did not follow the previous post, we can run the following command in the terminal first to pull a Docker image and then continue:\ndocker pull ubuntu:latest Now we are ready to proceed. To start the Docker container, we can start by typing the following command in the terminal\ndocker run ubuntu If all is correct, you should see, well nothing. But why is that? We just ran our Ubuntu image as a Docker container, but we don\u0026rsquo;t see anything on the terminal when we run it.\ndocker ps command To check whether a particular image is successfully run or not, we need to execute the following command in the terminal:\ndocker ps The docker ps command lists all the containers. So if you ran the Ubuntu image as per the previous step, then the docker ps command should show that the Ubuntu container is running. Let\u0026rsquo;s try to check if that is the case. In the terminal, execute the following command:\ndocker ps However, the output we see in the terminal should be the following:\n\u0026gt; docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES Hmm, weird right? The documentation says that the docker ps command gives us a list of the containers. But we do not see anything in the terminal. This is because the docker ps command only list the containers that are running by default. And for our Ubuntu image, we did not give any specific command to run when we started the container via the docker run command. Which is why it is not showing up under the docker ps command. To check if the container was ranning when we started it, we need to pass the -a flag to the docker ps command. Let\u0026rsquo;s run the following command in the terminal to check this:\n\u0026gt; docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 4c42acd7b1cd ubuntu \u0026#34;bash\u0026#34; 10 minutes ago Exited (0) 10 minutes ago condescending_villani We should see something like above in the terminal, with a different string under the NAMES and the CONTAINER ID column. Let\u0026rsquo;s see what terms shown in the output of the docker ps -a command mean.\nBelow is a list explaining what each of the terms given in the output above mean:\nCONTAINER ID: This column lists the id that a Docker container is given when it is run. This is a random id that is generated everytime a Docker container is run. If we run the same Docker image multiple times to start containers, it is very unlikely that the CONTAINER ID will be the same across different containers. IMAGE: This column gives the name of the Docker image that was used to start the Docker container. In our case, since we started the container using the command docker run ubuntu in the terminal, the IMAGE section correctly lists the ubuntu image in its run. COMMAND: This column lists the default command that is run when the Docker container is started. Since for the case of the ubuntu container, we did not specify any command, a default bash command is run on startup. CREATED: This field mentions the time when the container is started. STATUS: This field gives us a status of the container. In our case, we see the status code as being Exited (0) 10. This signifies that the ubuntu container was started, and is now shut down and is not running. The STATUS field helps us figure out the current status of a Docker container once it is started, and it can take different values, ranging from Creating ... to Up ..., which indicates that the container is either starting up or it is running for the amount of time mentioned after the Up string, respectively. PORTS: This field gives a list of ports that are to be exposed from inside the Docker container to the local user\u0026rsquo;s machine, so that the local user can access the port outside of Docker\u0026rsquo;s network NAMES: This field outputs the name that the docker container has when it is running. The name of a Docker container when it is run is also generated at random everytime a Docker container is started, but unlike the CONTAINER ID field, the name can be controlled and kept static for a particular Docker container. So, these are all the fields that are showed by the docker ps -a command. Now let\u0026rsquo;s start tampering with the docker run command to get a bit more out of the containers when we run them.\nPassing arguments to docker run command‚ÄºÔ∏è In the previous step, we just used the docker run command out of the box on the Ubuntu image. However, that did not give us much, not even a friendly terminal to let us inside the container that is running. Let\u0026rsquo;s now try to get inside the Ubuntu container to check what it has.\nTo do this, we first need to make the Ubuntu container execute a different command at runtime when it is started. For this, we can pass additional arguments to the docker run command in the terminal as follows:\ndocker run ubuntu sleep 3600 The sleep command that we pass is not a Docker argument, but a Linux system argument.\nTip: You can check more about the sleep command by running man sleep in your terminal (Only works for Linux-based OS like Ubuntu or MacOS)\nThe sleep command will override the default bash command from the Ubuntu container, and make the container sleep for the 3600 seconds 60 minutes. That should give us enough time to check what is inside the container.\nSo, once we have run the above command, we should see that the terminal does not exit like in the initial docker run command, but is in a way just stuck. However, if you open a new terminal and list all the containers with docker ps -a, we should now see the following:\n\u0026gt; docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 8ec5ceccbd72 ubuntu \u0026#34;sleep 3600\u0026#34; 3 seconds ago Up 1 second fervent_mclaren Notice how in the STATUS field it now shows Up 1 second. And also take a look COMMAND section to see that it now shows sleep 3600 as the command instead of bash like the previous case. That means the command that we passed while starting the container worked, and is executed as soon as the Ubuntu container is started. Let us now see what is inside the Ubuntu container.\nGoing inside a running container Now that the ubuntu container is running, we can look inside it. The default method to look what is inside a container is to get a terminal, and we can do the same for the ubuntu container. To get a terminal inside the container, we can run the following command:\n\u0026gt; docker exec -it \u0026lt;identifier\u0026gt; The exec command is used to run a command inside a running container. And to let Docker know which container we want to choose to execute the command, we need to pass an \u0026lt;identifier\u0026gt; above, which can either be the NAME of the container or the CONTAINER ID. Let\u0026rsquo;s pass the CONTAINER ID for now, but we can pass NAME in exactly the same way.\nPro Tip: For passing the CONTAINER ID as an identifier for the exec command, we do not need to pass the full container id. Instead, we can just pass the first 3 characters of the CONTAINER ID to the exec command, and it will still be executed inside the correct running container.\nTo execute and get a shell inside the running container, run the following command:\ndocker exec -it 8ec bash Please make sure to pass the correct first 3 characters from your CONTAINER ID, since every CONTAINER ID is randomly generated, and your CONTAINER ID will not be the same as mine.\nOnce we run the above command in a new terminal, we should see the following output:\n\u0026gt; docker exec -it 8ec bash root@8ec5ceccbd72:/# Well, well. We are now inside our running Docker container!!!üêï\nWe can now ls to check what files/directories the container has, and we can look around inside them as part of exploration. Let\u0026rsquo;s now see how we can give a name to a Docker container when we run it, and why is it useful.\nSome tips for running Docker containersüí° It is very beneficial to give your Docker container a name, since doing so will always maintain a streamlined process of seeing inside the container and accessing it for checking your processes. This can be done by passing in the --name flag to the docker run command, which is a Docker flag unlike the sleep command we saw earlier. Let\u0026rsquo;s execute the following command in the terminal: docker run --name my_ubuntu_image ubuntu sleep 1000 You can pass either my_ubutu_image as the name of your container, or you can get creative and pass something that you like as a name for your Docker container. Once we run the above command, you can open a new terminal and check running containers with docker ps -a. Once you do, you should now see something like the following output:\n\u0026gt; docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES f04164a6395f ubuntu \u0026#34;sleep 1000\u0026#34; 3 seconds ago Up 1 second my_ubuntu_image Notice how the docker container now has in the NAMES field the name my_ubuntu_image, or whatever name you passed along in the --name flag. Now if you want to exec inside the running container, instead of passing the CONTAINER ID, we can pass the name of the container and it will go inside. Let\u0026rsquo;s try this out with docker exec -it my_ubuntu_image bash to get bash inside the docker container. You should definitely see something like the following:\n\u0026gt; docker exec -it my_ubuntu_image bash root@f04164a6395f:/# Now we can always refer to our container while running commands with its name.\nNow that we have a Ubuntu container running, we need to know how to stop it. We need to stop old containers before running new ones, since Docker does not let us run two containers with the same name twice. To check this, run the following command again in a new terminal: docker run --name my_ubuntu_image ubuntu sleep 1000 You should get the following error message:\n\u0026gt; docker run --name my_ubuntu_image ubuntu docker: Error response from daemon: Conflict. The container name \u0026#34;/my_ubuntu_image\u0026#34; is already in use by container \u0026#34;f04164a6395f2c472bc5c6f6e59e304baa793b5d7edf106066e0764b51e1ad5d\u0026#34;. You have to remove (or rename) that container to be able to reuse that name. The error message tells us that there is a conflict with the container name, since we are already running a Ubuntu container with the same name. To rerun a new container with the same name, we first need to stop the one that is running currently. For this, we need to run the following command in a new terminal:\n\u0026gt; docker stop my_ubuntu_image Once you run this command, the terminal will be back after the command is executed successfully, and if you now get a list of all the containers, you should see the following:\n\u0026gt; docker ps -a CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES f04164a6395f ubuntu \u0026#34;sleep 1000\u0026#34; 12 minutes ago Exited (137) 9 seconds ago my_ubuntu_image As you can see, the STATUS field now shows Exited ... under it instead of the previously shown Up for x seconds. This means that our Ubuntu container has stopped running, and we are ready to start another container with the same desired name.\nAs per the above step, you now know how to stop a docker container. However, if you run the container again, you will still get the same error message with the name conflict as before. That is because docker not only needs you to stop the container, but to completely remove it before starting a new container. To do this, we can run the following command in a new terminal: docker system prune You should then show the following prompt in your terminal window:\n\u0026gt; docker system prune WARNING! This will remove: - all stopped containers - all networks not used by at least one container - all dangling images - all dangling build cache Are you sure you want to continue? [y/N] docker system prune simply removes all the unnecessary data from your Docker environment, like stopped containers, containers that could not start correctly, or cache that a container created on your machine when it was started. In the above prompt, type y and press enter to let docker clear all stopped containers and the data related to them. Once this is successful, you should see a message as follows:\nDeleted Containers: f04164a6395f2c472bc5c6f6e59e304baa793b5d7edf106066e0764b51e1ad5d Total reclaimed space: 98B This message indicates that all the stopped containers are now removed, alongwith the caches that they had created when started. To check if there are any containers still, you can run docker ps -a.\nIf you want to manually remove only one specific container from the stopped containers afters stopping it, you can run the following command instead:\ndocker rm \u0026lt;container name or id\u0026gt; This will remove only the container whose name or id you passed.\nThe methodology of running containers from pre-built docker images can be extended to running custom images as containers that we create according to the previous post, or any custom container of your choice. All the list of commands remain the same, only the pre-built ubuntu image from the above commands needs to be replaced with your own custom image.\nRunning multiple containers and make them interact with each other So far, we have seen how to run a single image as a container, give it a name, pass some commands to override the default commands, how to stop and remove it. However, large pieces of software are rarely built on only one running container, and there are always many containers handling one specific task of the software independent of other tasks that are run by the software.\nFor this, we need to run multiple containers at once, and these containers also need to be able to interact with each other in order to make the software run successfully. Once simple examples where different containers can be run to do different tasks is run a PostgreSQL database to store some results from an API, a container to run the API itself, and a pgAdmin UI tool to interact with the database to check if the data that the API returns are being stored successfully inside the postgreSQL database. This sounds like a very general purpose use-case that many companies have in common, and this can be managed very efficiently via docker containers. We will see more about this in the next post of Docker Compose files, which allows us to run multiple containers at once.\n","permalink":"https://tanmaychimurkar.github.io/posts/docker/docker_containers/","summary":"What is a Docker Container?üì¶ In the previous post, we saw how to find Docker images form the DockerHub, and also how to create our own Docker image that executes a function. In this post, we will go more over the execution part of Docker, mainly about how to run a Docker image as a container.\nDocker Containers are Docker images that become containers at run time. Docker containers are short pieces of software that runs an application quickly, as a lightweight standalone package.","title":"Docker Containers"}]